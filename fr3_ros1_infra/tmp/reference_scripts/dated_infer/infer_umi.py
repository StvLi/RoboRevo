import time
import requests
import numpy as np
import cv2
import base64
from scipy.spatial.transform import Rotation as R

# ================= æœåŠ¡å™¨äº¤äº’é…ç½® =================
# æ¨ç†æœåŠ¡å™¨åœ°å€é…ç½®
# å¦‚æœä½ åœ¨æœ¬æœºè·‘ï¼Œç”¨ "127.0.0.1"
# å¦‚æœä½ åœ¨å±€åŸŸç½‘å¦ä¸€å°æœºå™¨è·‘ï¼Œå¡«ä½  Mac çš„ IP (ä¾‹å¦‚ "172.16.17.208")
SERVER_IP = "172.16.17.208"  
SERVER_PORT = 5003
# æ¨ç†APIç«¯ç‚¹ï¼šPOSTè¯·æ±‚åˆ°æ­¤URLè·å–åŠ¨ä½œé¢„æµ‹
MODEL_URL = f"http://{SERVER_IP}:{SERVER_PORT}/predict_action"

# å›¾åƒå‚æ•°ï¼ˆç”¨äºé¢„å¤„ç†å’Œç¼–ç ï¼‰
IMG_WIDTH = 640
IMG_HEIGHT = 480
# æ¯æ¬¡æ¨ç†åæ‰§è¡Œçš„æ­¥æ•°ï¼ˆReceding Horizonç­–ç•¥ï¼šä¸€æ¬¡æ¨ç†ï¼Œæ‰§è¡Œå‰Næ­¥ï¼‰
EXEC_STEPS = 5
# ===========================================

# å°è¯•å¯¼å…¥ç¡¬ä»¶æ¥å£
try:
    from api_eef_gripper import CtrlFrankaAndGripper
    HAS_ROBOT_API = True
except ImportError:
    HAS_ROBOT_API = False
    print("âš ï¸ Warning: æ‰¾ä¸åˆ° api_eef_gripper.pyï¼Œå°†è¿è¡Œåœ¨ Mock (æ— ç¡¬ä»¶) æ¨¡å¼")

class RobotInferenceClient:
    def __init__(self, use_real_robot=True):
        self.use_real_robot = use_real_robot and HAS_ROBOT_API
        
        if self.use_real_robot:
            try:
                self.api = CtrlFrankaAndGripper()
                # æµ‹è¯•è¿æ¥
                self.api.get_eef_gripper()
                print("âœ… ç¡¬ä»¶è¿æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ç¡¬ä»¶è¿æ¥å¤±è´¥: {e}")
                self.use_real_robot = False
        else:
            self.api = None
            print("âš ï¸ æ­£åœ¨è¿è¡Œ Mock æ¨¡å¼ (ç”Ÿæˆéšæœºæ•°æ®)")

    def get_robot_state(self):
        """è·å–æœºæ¢°è‡‚çŠ¶æ€ [x,y,z, qx,qy,qz,qw, gripper_width]"""
        if not self.use_real_robot: 
            return np.array([0.5, 0.0, 0.5, 0, 0, 0, 1, 0.08])
        return np.array(self.api.get_eef_gripper(), dtype=float)

    def process_image(self, img_bgr):
        """BGR -> RGB -> Resize"""
        if img_bgr is None: 
            return np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (IMG_WIDTH, IMG_HEIGHT))
        return img_resized

    def encode_image_to_base64(self, img_rgb):
        """
        ã€æœåŠ¡å™¨äº¤äº’ - å›¾åƒç¼–ç ã€‘
        å°† Numpy RGB å›¾ç‰‡å‹ç¼©ä¸º JPG Base64 å­—ç¬¦ä¸²ï¼Œç”¨äºHTTPè¯·æ±‚ä¼ è¾“
        
        ä¼˜åŒ–ç›®çš„ï¼šå¤§å¹…å‡å°æ•°æ®åŒ…ä½“ç§¯ (åŸå§‹14MB -> å‹ç¼©å300KB)
        
        æµç¨‹ï¼š
        1. RGB -> BGR (OpenCVç¼–ç éœ€è¦BGRæ ¼å¼)
        2. JPEGå‹ç¼© (è´¨é‡95ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œå›¾åƒè´¨é‡)
        3. Base64ç¼–ç  (è½¬ä¸ºæ–‡æœ¬å­—ç¬¦ä¸²ï¼Œä¾¿äºJSONä¼ è¾“)
        
        Args:
            img_rgb: numpy array, shape (H, W, 3), dtype uint8, RGBæ ¼å¼
            
        Returns:
            str: Base64ç¼–ç çš„JPEGå›¾åƒå­—ç¬¦ä¸²
        """
        # è½¬å› BGR ç»™ OpenCV ç¼–ç 
        img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
        # ç¼–ç ä¸º JPG, è´¨é‡ 95
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]
        _, encimg = cv2.imencode('.jpg', img_bgr, encode_param)
        # è½¬ Base64 æ–‡æœ¬
        b64_string = base64.b64encode(encimg).decode('utf-8')
        return b64_string

    def get_images(self):
        """è·å–å›¾åƒåˆ—è¡¨ (List of Numpy RGB)"""
        if not self.use_real_robot:
            dummy = np.random.randint(0, 255, (IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)
            return [dummy, dummy, dummy] # æ¨¡æ‹Ÿä¸‰è§†è§’
            
        try:
            # è·å–ä¸‰å¼ åŸå§‹å›¾ (å‡è®¾é¡ºåº: Mid, Left, Right)
            # âš ï¸ è¯·ç¡®è®¤è¿™é‡Œè¿”å›çš„å˜é‡é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼
            img_mid, img_left, img_right = self.api.get_three_images(timeout_s=2.0)
            
            return [
                self.process_image(img_mid),
                self.process_image(img_left),
                self.process_image(img_right)
            ]
        except Exception as e:
            print(f"å–å›¾å¤±è´¥: {e}")
            blk = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)
            return [blk, blk, blk]

    def compute_target_pose(self, current_pose_8d, pred_action_7d):
        """
        ã€åŠ¨ä½œè§£æ - æœåŠ¡å™¨è¿”å›çš„Actionè½¬æ¢ä¸ºç›®æ ‡ä½å§¿ã€‘
        
        æœåŠ¡å™¨è¿”å›çš„actionæ˜¯ç›¸å¯¹äºå½“å‰æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»ï¼ˆLocal Frameï¼‰çš„å¢é‡ï¼Œ
        éœ€è¦è½¬æ¢ä¸ºä¸–ç•Œåæ ‡ç³»ï¼ˆGlobal Frameï¼‰ä¸‹çš„ç»å¯¹ç›®æ ‡ä½å§¿ã€‚
        
        Args:
            current_pose_8d: np.ndarray, shape=(8,), å½“å‰ä½å§¿
                [x, y, z, qx, qy, qz, qw, gripper_width]
            pred_action_7d: np.ndarray, shape=(7,), æœåŠ¡å™¨è¿”å›çš„åŠ¨ä½œå¢é‡
                [delta_x_local, delta_y_local, delta_z_local, 
                 rot_x, rot_y, rot_z, gripper_prob]
                - å‰3ç»´ï¼šä½ç½®å¢é‡ï¼ˆLocal Frameï¼Œå•ä½ï¼šç±³ï¼‰
                - ä¸­é—´3ç»´ï¼šæ—‹è½¬å¢é‡ï¼ˆæ—‹è½¬å‘é‡ï¼Œå•ä½ï¼šå¼§åº¦ï¼‰
                - æœ€å1ç»´ï¼šå¤¹çˆªå¼€åˆæ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
        
        Returns:
            np.ndarray, shape=(8,), ç›®æ ‡ä½å§¿
                [target_x, target_y, target_z, target_qx, target_qy, target_qz, target_qw, target_gripper_width]
        """
        # æå–å½“å‰ä½å§¿çš„ä½ç½®å’Œå››å…ƒæ•°
        curr_p = current_pose_8d[:3]  # ä½ç½® [x, y, z]
        curr_q = current_pose_8d[3:7]  # å››å…ƒæ•° [qx, qy, qz, qw]
        r_curr = R.from_quat(curr_q)  # è½¬æ¢ä¸ºæ—‹è½¬å¯¹è±¡

        # è§£ææœåŠ¡å™¨è¿”å›çš„åŠ¨ä½œ
        delta_p_local = pred_action_7d[:3]  # Local Frameä¸‹çš„ä½ç½®å¢é‡
        delta_rot_vec = pred_action_7d[3:6]  # æ—‹è½¬å‘é‡ï¼ˆæ—‹è½¬å¢é‡ï¼‰
        pred_gripper  = pred_action_7d[6]  # å¤¹çˆªå¼€åˆæ¦‚ç‡

        # åæ ‡è½¬æ¢ï¼šLocal Frame -> World Frame
        # å°†Local Frameä¸‹çš„ä½ç½®å¢é‡è½¬æ¢åˆ°World Frame
        delta_p_world = r_curr.apply(delta_p_local)
        target_p = curr_p + delta_p_world  # è®¡ç®—ç›®æ ‡ä½ç½®

        # å§¿æ€å åŠ ï¼šå°†æ—‹è½¬å¢é‡å åŠ åˆ°å½“å‰å§¿æ€
        r_rel = R.from_rotvec(delta_rot_vec)  # æ—‹è½¬å¢é‡è½¬ä¸ºæ—‹è½¬å¯¹è±¡
        r_next = r_curr * r_rel  # ç»„åˆæ—‹è½¬ï¼ˆå½“å‰å§¿æ€ * å¢é‡æ—‹è½¬ï¼‰
        target_q = r_next.as_quat()  # è½¬å›å››å…ƒæ•°æ ¼å¼

        # å¤¹çˆªæ˜ å°„ï¼šå°†æ¦‚ç‡å€¼ï¼ˆ0-1ï¼‰æ˜ å°„ä¸ºå®é™…å®½åº¦ï¼ˆç±³ï¼‰
        # é˜ˆå€¼0.5ï¼š>0.5è¡¨ç¤ºå¼ å¼€ï¼Œ<=0.5è¡¨ç¤ºé—­åˆ
        target_width = 0.08 if pred_gripper > 0.5 else 0.0
            
        return np.concatenate([target_p, target_q, [target_width]])

    def run(self, task_instruction, hz=5):
        print(f"=== å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_instruction} ===")
        print(f"=== ç›®æ ‡æœåŠ¡å™¨: {MODEL_URL} ===")
        dt = 1.0 / hz
        
        try:
            while True:
                t0 = time.time()
                
                # ============================================================
                # ã€æœåŠ¡å™¨äº¤äº’æµç¨‹ - è¯·æ±‚é˜¶æ®µã€‘
                # ============================================================
                
                # 1. æ•°æ®é‡‡é›†ï¼šè·å–å¤šè§†è§’å›¾åƒå’Œæœºå™¨äººçŠ¶æ€
                images_np = self.get_images()  # List[np.ndarray], æ¯ä¸ªå…ƒç´  shape=(H, W, 3), RGBæ ¼å¼
                start_state = self.get_robot_state()  # np.ndarray, shape=(8,), [x,y,z, qx,qy,qz,qw, gripper_width]
                
                # 2. å›¾åƒç¼–ç ï¼šå°†numpyæ•°ç»„è½¬ä¸ºBase64å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆç”¨äºHTTPä¼ è¾“ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œåªç¼–ç å›¾åƒï¼Œstateåœ¨payloadä¸­è®¾ä¸ºNoneï¼ˆæœåŠ¡ç«¯ä¼šè‡ªåŠ¨è¡¥å…¨ä¸º0ï¼‰
                images_b64 = [self.encode_image_to_base64(img) for img in images_np]
                
                # 3. æ„é€ è¯·æ±‚Payload
                # è¯·æ±‚æ ¼å¼è¯´æ˜ï¼š
                # - "examples": åˆ—è¡¨æ ¼å¼ï¼Œæ”¯æŒæ‰¹é‡æ¨ç†ï¼ˆè¿™é‡Œåªä¼ ä¸€ä¸ªæ ·æœ¬ï¼‰
                # - "image": Base64ç¼–ç çš„å›¾åƒåˆ—è¡¨ï¼Œé¡ºåºéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ˆé€šå¸¸ä¸º [Mid, Left, Right]ï¼‰
                # - "lang": ä»»åŠ¡æŒ‡ä»¤æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€æè¿°ï¼‰
                # - "state": æœºå™¨äººçŠ¶æ€ï¼Œè¿™é‡Œä¼ Noneè¡¨ç¤ºæœåŠ¡ç«¯ä¼šè‡ªåŠ¨è¡¥å…¨ä¸ºå…¨0å‘é‡
                #            ï¼ˆå¦‚æœéœ€è¦ä¼ çœŸå®stateï¼Œæ ¼å¼åº”ä¸º: [x, y, z, qx, qy, qz, qw, gripper_width]ï¼‰
                payload = {
                    "examples": [
                        {
                            "image": images_b64,     # List[str]: Base64ç¼–ç çš„JPEGå›¾åƒå­—ç¬¦ä¸²åˆ—è¡¨
                            "lang": task_instruction, # str: ä»»åŠ¡æŒ‡ä»¤ï¼Œå¦‚ "Pick up the banana"
                            "state": None            # Optional[List[float]]: ä¼ Noneæ—¶æœåŠ¡ç«¯è‡ªåŠ¨è¡¥å…¨ä¸º0
                        }
                    ]
                }
                
                # 4. å‘é€HTTP POSTè¯·æ±‚åˆ°æ¨ç†æœåŠ¡å™¨
                try:
                    # è¯·æ±‚å‚æ•°ï¼š
                    # - url: MODEL_URL (ä¾‹å¦‚ "http://172.16.17.208:5003/predict_action")
                    # - json: è‡ªåŠ¨åºåˆ—åŒ–ä¸ºJSONæ ¼å¼çš„è¯·æ±‚ä½“
                    # - timeout: 30ç§’è¶…æ—¶ï¼ˆæ¨ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
                    resp = requests.post(MODEL_URL, json=payload, timeout=30.0)
                    
                    # 5. æ£€æŸ¥HTTPå“åº”çŠ¶æ€
                    if resp.status_code != 200:
                        print(f"Server Error {resp.status_code}: {resp.text}")
                        continue  # è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡å¾ªç¯
                        
                    # 6. è§£æJSONå“åº”
                    result = resp.json()
                    
                    # ============================================================
                    # ã€æœåŠ¡å™¨äº¤äº’æµç¨‹ - å“åº”è§£æé˜¶æ®µã€‘
                    # ============================================================
                    
                    # å“åº”æ•°æ®ç»“æ„è¯´æ˜ï¼š
                    # result["data"]["unnormalized_actions"] çš„å½¢çŠ¶ä¸º [Batch=1, Chunk=30, Dim=7]
                    # - Batch: æ‰¹æ¬¡å¤§å°ï¼ˆè¿™é‡Œä¸º1ï¼Œå› ä¸ºåªä¼ äº†ä¸€ä¸ªæ ·æœ¬ï¼‰
                    # - Chunk: é¢„æµ‹çš„åŠ¨ä½œåºåˆ—é•¿åº¦ï¼ˆé€šå¸¸ä¸º30æ­¥ï¼ŒReceding Horizonç­–ç•¥ï¼‰
                    # - Dim: åŠ¨ä½œç»´åº¦ï¼ˆ7ç»´ï¼š3ç»´ä½ç½®å¢é‡ + 3ç»´æ—‹è½¬å¢é‡ + 1ç»´å¤¹çˆªï¼‰
                    #   åŠ¨ä½œæ ¼å¼: [delta_x, delta_y, delta_z, rot_x, rot_y, rot_z, gripper]
                    #   æ³¨æ„ï¼šä½ç½®å’Œæ—‹è½¬éƒ½æ˜¯ç›¸å¯¹äºå½“å‰æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»ï¼ˆLocal Frameï¼‰çš„å¢é‡
                    
                    all_actions = np.array(result["data"]["unnormalized_actions"])
                    # æå–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç´¢å¼•0ï¼‰çš„å‰EXEC_STEPSæ­¥åŠ¨ä½œ
                    # æœ€ç»ˆ shape: (EXEC_STEPS, 7)
                    action_chunk = all_actions[0, :EXEC_STEPS, :]
                    
                except Exception as e:
                    # è¯·æ±‚å¼‚å¸¸å¤„ç†ï¼šç½‘ç»œé”™è¯¯ã€è¶…æ—¶ã€JSONè§£æé”™è¯¯ç­‰
                    print(f"âŒ Inference Failed: {e}")
                    time.sleep(0.5)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                    continue

                # ==========================
                # 2. æ‰§è¡Œé˜¶æ®µ (Execution)
                # ==========================
                print(f"ğŸš€ Executing Chunk ({len(action_chunk)} steps)...")
                
                # åˆå§‹åŒ–â€œè™šæ‹Ÿå½“å‰çŠ¶æ€â€ï¼Œç”¨äºé“¾å¼è®¡ç®—
                virtual_current_pose = start_state.copy()
                
                for i, action in enumerate(action_chunk):
                    t_step_start = time.time()
                    
                    # åŸºäºä¸Šä¸€æ­¥çš„è™šæ‹Ÿç»ˆç‚¹ï¼Œè®¡ç®—è¿™ä¸€æ­¥çš„ç»ˆç‚¹
                    target_pose = self.compute_target_pose(virtual_current_pose, action)
                    
                    # å®‰å…¨æ£€æŸ¥ (é˜²æ­¢å•æ­¥çªå˜)
                    step_diff = np.linalg.norm(target_pose[:3] - virtual_current_pose[:3])
                    if step_diff > 0.05: # å•æ­¥è¶…è¿‡ 5cm
                         print(f"âš ï¸ Step {i} jump too large ({step_diff:.3f}m), clamping...")
                         # å¯ä»¥åœ¨è¿™é‡Œåšæ’å€¼é™åˆ¶ï¼Œè¿™é‡Œç®€å•è·³è¿‡æ‰“å°è­¦å‘Š
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    # print(f"   Step {i+1}: Pos={target_pose[:3].round(3)}")
                    if self.use_real_robot:
                        self.api.set_eef_gripper(target_pose.tolist())
                    
                    # ã€é‡è¦ã€‘æ›´æ–°è™šæ‹ŸçŠ¶æ€ï¼Œä¸ºä¸‹ä¸€æ­¥åšå‡†å¤‡
                    virtual_current_pose = target_pose
                    
                    # ç»´æŒ Hz
                    elapsed = time.time() - t_step_start
                    time.sleep(max(0, dt - elapsed))

        except KeyboardInterrupt:
            print("\nğŸ›‘ åœæ­¢æ‰§è¡Œ")


if __name__ == "__main__":
    client = RobotInferenceClient(use_real_robot=True)
    client.run("Pick up the banana and put it in the basket")